\section{Compression algorithms}



Data compression is increasingly used to reduce the memory footprint of
scientific applications. General purpose data compression algorithms like Zlib
(which is a part of gzip)~\cite{deutsch1996zlib}, and compression algorithms for
video and image data such as JPEG-2000~\cite{skodras2001jpeg} have been
presented in previous work. More recently, special purpose compression
algorithms for floating-point scientific data have been developed, such as ZFP
or SZ~\cite{lindstrom2014fixed,di2018efficient}.

Lossless algorithms guarantee that the exact original data can be recovered
during decompression, whereas lossy algorithms introduce an error, but often
guarantee that the error does not exceed certain absolute or relative error
metrics. Typically, lossy compression is more effective in reducing the data
size. Most popular compression packages offer various settings that allow a
tradeoff between compression ratio, accuracy, and compression and decompression
time.

Another comonly-observed difference between lossless and lossy
compression algorithms is that lossless compression algorithms tend to
interpret all data as one-dimensional series only while SZ and ZFP,
being designed for scientific data, tend to take the dimensionality
into account directly. This makes a difference in the case of a
wavefield, for example, where the data to be compressed corresponds to
a smoothly varying function in (two or) three dimensions and
interpreting this three-dimensional data as one-dimensional would
completely miss the smoothness and predictability of the data values.

It is worth noting that another data reduction strategy is to typecast values
into a lower precision format, for example, from double precision to single
precision. This can be seen as a computationally cheap lossy compression
algorithm with a compression ratio of $2$.

Perhaps counterintuitively, compression can not only reduce the memory
footprint, but also speed up an application. Previous work has observed that the
compression and decompression time can be less than the time saved from the
reduction in data that needs to be communicated across MPI nodes or between a
GPU and a host computer~\cite{gpu-compression}.

One way of using compression in adjoint-based methods is to compress
all the timesteps during the forward pass. If the compression ratio is
sufficient to fit the entire data in memory, this enables solving an
adjoint-based optimisation problem without resorting to any of the
other techniques previously discussed here. Specifically, compression
serves as an \emph{alternate strategy} to checkpointing in this
scenario. Previous work has discussed this in the context of
computational fluid dynamics~\cite{cyr2015towards,marin2016large} and seismic
inversion using compression algorithms specifically designed for
the respective applications~\cite{dalmau2014lossy,boehm2016wavefield}. 

Since the time spent on compressing and decompressing data is often
non-negligible, this raises the question whether the computational
time is better spent on this compression and decompression, or on the
recomputation involved in the more traditional checkpointing
approach. This question was previously answered to a limited extent
for the above scenario where compression is an alternative to
checkpointing, in a specific application~\cite{cyr2015towards}. We discuss
that in section TODO. 

\subsection{Lossless}
We use the python package \emph{blosc}~\cite{blosc}, which includes implementations for
six different lossless compression algorithms, namely ZLIB, ZSTD, BLOSCLZ,
LZ4, LZ4HC and Snappy. All these algorithms look at the data as a one-dimensional stream of bits
and at least the blosc implementations have a limit on the size of the one-dimensional array that
can be compressed in one call. Therefore we use the python package \emph{blosc-pack}, which is
a wrapper over the blosc library, to implement \emph{chunking}, i.e. breaking up the stream into
chunks of a chosen size, which are compressed one at a time. 

\subsection{Lossy}
\subsubsection{ZFP}
We use the lossy compression package ZFP~\cite{lindstrom2014fixed} developed in
C. To use ZFP from python, we developed a python wrapper for the reference
implementation of ZFP \footnote{To be released open source on publication}.

ZFP supports three compression modes, namely fixed-tolerance, fixed-precision
and fixed-rate. The fixed-tolerance mode limits the absolute error, while the
fixed-precision mode limits the error as a ratio of the range of values in the array to be compressed.
The fixed-rate mode achieves a guaranteed compression ratio requested by the
user, but does not provide any bounds on accuracy loss.


\subsection{SZ}
SZ~\cite{di2018efficient} is a more recently developed compression library, also focussed on lossy compression
of floating-point scientific data, also developed in C. We also wrote a python wrapper for the reference
implementation of SZ to use it as part of our benchmark suite. \footnote{Also to be released open source
upon publication}

SZ supports four compression modes, namely absolute error mode, which, similar to ZFP's fixed-tolerance
mode, allows the user to control the maximum pointwise error in absolute values. The relative ratio mode 
of SZ allows the user to specify a maximum error as a ratio of the range of values in the array, which is
effectively similar to ZFP's fixed-precision mode but not exactly. SZ has two other modes that are missing 
in ZFP, namely pointwise relative error and pointwise SNR mode. In the pointwise relative error mode, the user
can provide a relative error ratio and SZ will ensure that the error at each point is within that ratio, considering
its absolute value. In the pointwise SNR mode, the user provides a signal-to-noise ratio value that SZ respects
at each point.

\subsection{Combining lossy and lossless compression}
Another approach we attempted is a combination of lossy and lossless compression 
schemes to achieve an overall lossless compression scheme. Here, the array is first compressed using a 
lossy compression scheme, following which the errors incurred by this lossy scheme are passed on to a 
lossless scheme for compression. The idea is that the distribution of errors incurred by a lossy compression
algorithm might make it more favourable for lossless compression than the original array.